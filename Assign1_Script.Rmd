---
title: "Symbiont Communities of *Siderastrea siderea* and *Siderastrea radians*"
author: "Alexa Huzar, Jacob Jaskiel, Nicholas Peoples"
date: "2/27/2021"
output: html_document
---

# Introduction
We compared the algal symbiont community composition from two different species in the family *Siderastreidae*. In order to this, we are using ITS2 sequence data from Nicola Kriefall. 

**R version** 

We used R version R-4.0.3

# Pre-processing of reads
The fastq files have R1 (forward read) and R2 (reverse read) designations for pair end data

# Dada2 pipeline steps

# First we are going to load our libraries and set our working directory
```{r, eval=FALSE}
library(dada2); #packageVersion("dada2"); citation("dada2")
library(ShortRead); #packageVersion("ShortRead")
library(ggplot2); #packageVersion("ggplot2")
library(phyloseq); #packageVersion("phyloseq")
library(R.utils)
```

# Next we are goind to set path to unzipped fastq files. We are only using a subset of the data in this anaylsis for ease
```{r, eval=FALSE}
path <- "C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads" 
fns <- list.files(path)
fns
```




################################
##### Trimming/Filtering #######
################################

```{r}
fastqs <- fns[grepl(".fastq", fns)]
fastqs <- sort(fastqs) # Sort ensures reads are in same order
```

# Get sample names, assuming files named as so: SAMPLENAME_XXX.fastq; OTHERWISE MODIFY
```{r}
sample.names <- sapply(strsplit(fastqs, ".fastq"), `[`, 1) 
sample.names
# Specify the full path to the fnFs
fnFs <- file.path(path, fastqs)
fnFs
```

```{r}
fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))
```

```{r}
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(fnFs, get.sample.name))
head(sample.names)
sample.names
```

#### check for primers ####
```{r}
FWD <- "GTGAATTGCAGAACTCCGTG"  ## CHANGE ME to your forward primer sequence
REV <- "CCTCCGCTTACTTATATGCTT"  ## CHANGE ME...
```


allOrients <- function(primer) {
  # Create all orientations of the input sequence
  require(Biostrings)
  dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients

fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)

primerHits <- function(primer, fn) {
  # Counts number of reads in which the primer is found
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[2]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[2]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[2]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[2]]))

#########Visualize Raw data

#First, lets look at quality profile of R1 reads
```{r}
plotQualityProfile(fnFs[c(1,2,3,4,5,6,7,8,9)])
plotQualityProfile(fnFs[c(10,12,13,15,16,17)])
plotQualityProfile(fnRs[c(1,2,3,4,5,6,7,8,9)])
plotQualityProfile(fnRs[c(10,12,13,15,16,17)])
```



#Recommend trimming where quality profile crashes - in this case, forward reads mostly fine up to 300
#For common ITS amplicon strategies with paired end reads, it is undesirable to truncate reads to a fixed length due to the large amount of length variation at that locus. That is OK, just leave out truncLen. Make sure you removed the forward and reverse primers from both the forward and reverse reads though! 

#Make directory and filenames for the filtered fastqs
```{r}
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq"))
```

# Filter
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(220,200), #leaves overlap
                     maxN=0, #DADA does not allow Ns
                     maxEE=c(1,1), #allow 1 expected errors, where EE = sum(10^(-Q/10)); more conservative, model converges
                     truncQ=2, 
                     minLen = 50,
                     #trimLeft=c(20,21), #N nucleotides to remove from the start of each read
                     rm.phix=TRUE, #remove reads matching phiX genome
                     matchIDs=TRUE, #enforce mtching between id-line sequence identifiers of F and R reads
                     compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
```

head(out)
tail(out)

#A word on Expected Errors vs a blanket quality threshold
#Take a simple example: a read of length two with quality scores Q3 and Q40, corresponding to error probabilities P=0.5 and P=0.0001. The base with Q3 is much more likely to have an error than the base with Q40 (0.5/0.0001 = 5,000 times more likely), so we can ignore the Q40 base to a good approximation. Consider a large sample of reads with (Q3, Q40), then approximately half of them will have an error (because of the P=0.5 from the Q2 base). We express this by saying that the expected number of errors in a read with quality scores (Q3, Q40) is 0.5.
#As this example shows, low Q scores (high error probabilities) dominate expected errors, but this information is lost by averaging if low Qs appear in a read with mostly high Q scores. This explains why expected errors is a much better indicator of read accuracy than average Q.

################################
##### Learn Error Rates #######
################################
#DADA2 learns its error model from the data itself by alternating estimation of the error rates and the composition of the sample until they converge on a jointly consistent solution (this is similar to the E-M algorithm)
#As in many optimization problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

#setDadaOpt(MAX_CONSIST=30) #if necessary, increase number of cycles to allow convergence
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
```
#sanity check: visualize estimated error rates
#error rates should decline with increasing qual score
#red line is based on definition of quality score alone
#black line is estimated error rate after convergence
#dots are observed error rate for each quality score
```{r}
plotErrors(errF, nominalQ=TRUE) 
plotErrors(errR, nominalQ=TRUE) 
```

################################
##### Dereplicate reads #######
################################
#Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance”: the number of reads with that unique sequence. 
#Dereplication substantially reduces computation time by eliminating redundant comparisons.
#DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent denoising step, significantly increasing DADA2’s accuracy.
```{r}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
```
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

################################
##### Infer Sequence Variants #######
################################
```{r}
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
```

#now, look at the dada class objects by sample
#will tell how many 'real' variants in unique input seqs
#By default, the dada function processes each sample independently, but pooled processing is available with pool=TRUE and that may give better results for low sampling depths at the cost of increased computation time. See our discussion about pooling samples for sample inference. 
```{r}
dadaFs[[1]]
dadaRs[[1]]
```

#~############################~#
##### Merge paired reads #######
#~############################~#

#To further cull spurious sequence variants
#Merge the denoised forward and reverse reads
#Paired reads that do not exactly overlap are removed

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

summary((mergers[[1]]))
```

#We now have a data.frame for each sample with the merged $sequence, its $abundance, and the indices of the merged $forward and $reverse denoised sequences. Paired reads that did not exactly overlap were removed by mergePairs.

#~##################################~#
##### Construct sequence table #######
#~##################################~#
#a higher-resolution version of the “OTU table” produced by classical methods

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
rowSums(seqtab)
```

# Inspect distribution of sequence lengths
```{r}
table(nchar(getSequences(seqtab)))
```
#mostly at 300 bp, which is just what was expected [271-303 bp]
```{r}
plot(table(nchar(getSequences(seqtab))))
```

#The sequence table is a matrix with rows corresponding to (and named by) the samples, and 
#columns corresponding to (and named by) the sequence variants. 
#Sequences that are much longer or shorter than expected may be the result of non-specific priming, and may be worth removing

#if I wanted to remove some lengths - not recommended by dada2 for its2 data
#seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(268,327)] 

```{r}
table(nchar(getSequences(seqtab)))
dim(seqtab)
plot(table(nchar(getSequences(seqtab))))
```

#construct sequence table
```{r}
seqtab <- makeSequenceTable(dadaFs)
head(seqtab)
```

################################
##### Remove chimeras #######
################################
#The core dada method removes substitution and indel errors, but chimeras remain. 
#Fortunately, the accuracy of the sequences after denoising makes identifying chimeras easier 
#than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as 
#a bimera (two-parent chimera) from more abundant sequences.

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
#Identified 38 bimeras out of 156 input sequences.

```{r}
sum(seqtab.nochim)/sum(seqtab)
```
#0.9989
#The fraction of chimeras varies based on factors including experimental procedures and sample complexity, 
#but can be substantial. 
################################
##### Track Read Stats #######
################################

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
head(track)
tail(track)
```
```{r}
write.csv(track,file="its2_reads.csv",row.names=TRUE,quote=FALSE)
```

#plotting for no reason

#manually added raw counts from the raw .fastq file using the following loop in Terminal:
# for file in *R1_001.fastq
# do
# echo $file >> raw_r1_names
# grep @M0 $file | wc -l >> raw_r1_counts
# done
setwd("~/moorea_holobiont/mr_ITS2/")
reads <- read.csv("its2_reads_renamed.csv")
#counts1 = raw
#counts2 = input
#counts3 = filtered
#counts4 = denoised
#counts5 = merged
#counts6 = nonchim

reread <- reshape(reads, varying = c("counts1","counts2", "counts3", "counts4", "counts5", "counts6"), timevar = "day",idvar = "sample", direction = "long", sep = "")
reread$sample <- as.factor(reread$sample)
ggplot(reread,aes(x=day,y=counts,color=sample))+
  geom_point()+
  geom_path()

library(Rmisc)
reread.se <- summarySE(data=reread,measurevar="counts",groupvars=c("day"))
ggplot(reread.se,aes(x=day,y=counts))+
  geom_point()+
  geom_path()+
  geom_errorbar(aes(ymin=counts-se,ymax=counts+se),width=0.3)


################################
##### Assign Taxonomy #######
################################

#It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to classify sequence variants taxonomically. 
#DADA2 provides a native implementation of the RDP's naive Bayesian classifier. The assignTaxonomy function takes a set of sequences and a training set of taxonomically classified sequences, and outputs the taxonomic assignments with at least minBoot bootstrap confidence.
#Here, I have supplied a modified version of the GeoSymbio ITS2 database listing more taxonomic info as phyloseq requires (Franklin et al. 2012)
#For example: GeoSymbio data (taken from "all clades" at https://sites.google.com/site/geosymbio/downloads):
#>A1.1
#modified version for phyloseq looks like this instead:
#>Symbiodinium; Clade A; A1.1

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "GeoSymbio_ITS2_LocalDatabase_verForPhyloseq.fasta",tryRC=TRUE,minBoot=70,verbose=TRUE)
unname(head(taxa))
```

#to come back to later
```{r}
saveRDS(seqtab.nochim, file="C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads/mrits2_seqtab.nochim.rds")
saveRDS(taxa, file="C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads/mrits2_taxa.rds")
```

```{r}
write.csv(seqtab.nochim, file="mrits2_seqtab.nochim.csv")
write.csv(taxa, file="C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads/mrits2_taxa.csv")
```

#### Reading in prior data files ####
```{r}
setwd("C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads")
seqtab.nochim <- readRDS("mrits2_seqtab.nochim.rds")
taxa <- readRDS("mrits2_taxa.rds")
```


################################
##### handoff 2 phyloseq #######
################################

#import dataframe holding sample information
#have your samples in the same order as the seqtab file in the rows, variables as columns
samdf<-read.csv("variabletable.csv")
head(samdf)
head(seqtab.nochim)
head(taxa)
rownames(samdf) <- samdf$sample

# Construct phyloseq object (straightforward from dada2 outputs)
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
ps

#replace sequences with shorter names (correspondence table output below)
ids<-taxa_names(ps)
ids <- paste0("sq",seq(1, length(colnames(seqtab.nochim))))
colnames(seqtab.nochim) <- ids

#Bar-plots
top90 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:90]
ps.top90 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top90 <- prune_taxa(top90, ps.top90)

plot_bar(ps.top90, x="Sample", fill="Class") 

#visusalize via counts rather than abundances:
plot_bar(ps, x = "sample", fill= "Class") #+ facet_wrap("tank")
#
#Obtain a csv file for the phyloseq data. Will give you the abundances for each sample and class. Useful for constructing the heatmap. Also, enables you to use ggplot, and construct more aesthetically pleasing plot.
psz <- psmelt(ps.top90)
write.csv(psz, file="Phyloseqoutputfinal.csv")
p <- ggplot(psz, aes(x=Sample, y=Abundance, fill=Class))
p + geom_bar(stat="identity", colour="black")
