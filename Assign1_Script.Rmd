---
title: "Symbiont Communities of *Siderastrea siderea* and *Siderastrea radians*"
author: "Alexa Huzar, Jacob Jaskiel, Nicholas Peoples"
date: "2/27/2021"
output: html_document
---

# Introduction
We compared the algal symbiont community composition from two different species in the family *Siderastreidae* collected off the coast of Florida just before a hurricane. In order to accomplish this, we are using ITS2 sequence data from Nicola Kriefall with the Dada2 Pipeline in R Studio. 

**R version** 

We used R version R-4.0.3

The fastq files have R1 (forward read) and R2 (reverse read) designations for pair end data

# Dada2 Pipeline Steps
**In order to do our anaylsis of the communities, we used the dada2 pipeline. This pipeline allows us to trim and filter our sequences and merge our paired reads before assinging taxonomy to assess what species were present.**

### Preparation Steps
**We first loaded the appropriate libraries for all the analysis and graphs.**

```{r, loadlib, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
library(dada2);
library(ggplot2);
library(ShortRead); 
library(ggplot2); 
library(phyloseq); 
library(Rmisc)
```

**Additonally we set the path to be the location of the unzipped fastq files**
```{r}
path <- "C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads" 
fns <- list.files(path)
```

**These are the files in our path, including all of the unzipped fastq files**
```{r}
fns
```

**Now we sort the files by forward and reverse sequence reads**

```{r}
fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))
```

**After sorting, we can get the samples names. Note that name format should be SAMPLENAME_XXX.fastq**

```{r}
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(fnFs, get.sample.name))
sample.names
```
### Checking for Primers

**We checked for primers to ensure that we did not need to remove them from the reads**

These are our primers

```{r}
FWD <- "GTGAATTGCAGAACTCCGTG"
REV <- "CCTCCGCTTACTTATATGCTT" 
```

**Create all orientations of the primer sequences**
```{r}
allOrients <- function(primer) {
  require(Biostrings)
  dna <- DNAString(primer)
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))
  }

```

Then using the function set up above we create the orientations of our specific primer sequences in order to check all our sequences
```{r, echo=TRUE, results = 'hide'}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients
```
Here we create a subdirectory of the filtered sequences for the next step to check for primers
```{r}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs))
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = FALSE)
```
Now, we create a function to check for the presence of our forward or reverse primers in forward and reverse reads. 
```{r}
primerHits <- function(primer, fn) {
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[2]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[2]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[2]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[2]]))
```
We found no primers, so we can proceed.

### Visualize Raw data

**Now we check the quality of the reads to determine at what point we should trim to avoid errors**

As our files are paired end we ran quality plots for both the forward and reverse reads. We had to remove two sequences due to poor quality

#### Example of our foward read plots
```{r, echo=TRUE, results = 'hide'}
plotQualityProfile(fnFs[c(1,2,3,4,5,6,7,8,9)])
plotQualityProfile(fnFs[c(10,11,12,13,14,15)])
```

```{r,echo=FALSE}
plotQualityProfile(fnFs[c(1,8)])
```

#### Examples of our reverse read plots
```{r, echo=TRUE, results = 'hide'}
plotQualityProfile(fnRs[c(1,2,3,4,5,6,7,8,9)])
plotQualityProfile(fnRs[c(10,11,12,13,14,15)])
```

```{r, echo=FALSE}
plotQualityProfile(fnRs[c(1,8)])
```


**Make directory and filenames for the filtered fastqs**
```{r}
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq"))
```

Now, that we set the path we can actually filter our reads based on the quality plots. We are using the parameters set by Nicola as our data was from her. The truncLen values allow trimming to get better quality while maintaining overlap for later merging. We use these truncLen values as the quality of our reads drops off after about 220
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(220,200),
                     maxN=0,
                     maxEE=c(1,1),
                     truncQ=2, 
                     minLen = 50,
                     rm.phix=TRUE, 
                     matchIDs=TRUE, 
                     compress=TRUE, multithread=FALSE)
head(out)
tail(out)
```

### Error Rates

DADA2 learns its error model from the data itself by alternating estimation of the error rates and the composition of the sample until they converge on a jointly consistent solution (this is similar to the E-M algorithm). As in many optimization problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
```

**This is a quick sanity check of our error rates via visualization**
The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score
```{r, warning = FALSE}
plotErrors(errF, nominalQ=TRUE) 
plotErrors(errR, nominalQ=TRUE) 
```

Error rates converge to zero with increasing Consensus Quality Score, as we had hoped


### Dereplicate reads
**This is important to reduce computation time by combining all identical sequences into unique sequences with their corresponding abundance**
 
```{r,echo=TRUE, results='hide', message=FALSE, warning=FALSE}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
```
Name the derep-class objects by the sample names
```{r}
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```


### Infer Sequence Variants

```{r, echo=TRUE, results = 'hide'}
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
```
The above code takes the dereplicated amplicon sequencing reads and returns the inferred composition of the samples, essentially removing sequencing errors.


```{r}
dadaFs[[1]]
dadaRs[[1]]
```
These are the number of real variants out of all the unique input sequences in the forward and reverse read of our first sample


### Merge paired reads

**Now we merge our forward and reverse reads to obtain the full denoised sequences. The forward read is aligned with the reverse complement of the reverse read**

The output is only kept if there is at least a 12 base overlap and the overlap is identical

```{r, echo=TRUE, results = 'hide'}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
```

```{r}
head(mergers[[1]])
summary((mergers[[1]]))
```


### Construct sequence table

**This is our ASV table which is a higher resolution OTU**

Unlike an OTU table, it requires 100% identity threshold.
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```
Our 15 samples had 180 ASVs

# Inspect distribution of sequence lengths
```{r}
table(nchar(getSequences(seqtab)))
```

Most reads were at 306 bp (73 datapoints), which is close to what was expected according to Nicola's data, which assumed a range of 271-303 bp. The plot below shows the frequency of these sequences by base pairs

```{r}
plot(table(nchar(getSequences(seqtab))))
```

Based on the table and plot none of sequences need to be removed as none are longer or shorter than expected


### Remove chimeras

**The dada pipeline has already removed substitution and indel errors, but chimeras still need to be removed**

Here, we remove all chimeras and return chimera-free unique sequences 
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```

Now, we find the proportion of our reads that do not have chimeras.
```{r}
sum(seqtab.nochim)/sum(seqtab)
```



### Track Read Stats

Here, we create a table to visualize our reads as they were filtered, denoised, merged, tabled, and filtered for chimeras
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
head(track)
tail(track)
```
Here we write this table into a .csv file
```{r}
write.csv(track,file="its2_reads.csv",row.names=TRUE,quote=FALSE)
```



### Assign Taxonomy

**Now we can assign taxonomy based on comparing our sequences to the GeoSymbio database**

Note that while there is a reclassification of Symbiodiniaceae family, the database still refers to it as clades while genus is now the accepted assignment



This assigns a clade to each sequence using corresponding reads from the GeoSymbio database
```{r, echo=TRUE, results = 'hide'}
taxa <- assignTaxonomy(seqtab.nochim, "GeoSymbio_ITS2_LocalDatabase_verForPhyloseq.fasta",tryRC=TRUE,minBoot=70,verbose=TRUE)
(head(taxa))
```

The code below writes these objects as a file so we can restore and come back to it later, as needed
```{r}
saveRDS(seqtab.nochim, file="C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads/mrits2_seqtab.nochim.rds")
saveRDS(taxa, file="C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads/mrits2_taxa.rds")
write.csv(seqtab.nochim, file="mrits2_seqtab.nochim.csv")
write.csv(taxa, file="C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads/mrits2_taxa.csv")
```


# Handoff 2 phyloseq

First we import in the dataframe holding sample information from Nicola's github
```{r, echo=TRUE, results = 'hide'}
samdf<-read.csv("mrits_sampledata.csv")
head(samdf)
rownames(samdf) <- samdf$Sample
```


Phyloseq object with shorter names
```{r}
ids <- paste0("sq", seq(1, length(colnames(seqtab.nochim))))
```

We now construct a phyloseq object directly from the dada2 outputs.
```{r}
colnames(seqtab.nochim)<-ids
taxa2 <- cbind(taxa, rownames(taxa)) 
rownames(taxa2)<-ids

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_names(samdf), 
               tax_table(taxa2))
```

```{r}
ps
```


# Bar-plots
```{r}
top90 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:90]
ps.top90 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top90 <- prune_taxa(top90, ps.top90)

plot_bar(ps.top90, x="Sample", fill="Phylum") 
```

visusalize via counts rather than abundances:
```{r}
plot_bar(ps, x= "Sample", fill= "Phylum")
```






