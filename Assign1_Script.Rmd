---
title: "Symbiont Communities of *Siderastrea siderea* and *Siderastrea radians*"
author: "Alexa Huzar, Jacob Jaskiel, Nicholas Peoples"
date: "2/27/2021"
output: html_document
---

# Introduction
We compared the algal symbiont community composition from two different species in the family *Siderastreidae*. In order to this, we are using ITS2 sequence data from Nicola Kriefall. 

**R version** 

We used R version R-4.0.3

The fastq files have R1 (forward read) and R2 (reverse read) designations for pair end data

# Dada2 Pipeline Steps
**In order to do our anaylsis of the communities, we used the dada2 pipeline. This pipeline allows us to trim and filter our sequences and merge our paired reads before assinging taxonomy to assess what species were present. 

### Preparation Steps
**We first loaded the appropriate libraries for all the analysis and graphs.**

```{r, eval=FALSE}
library(dada2);
library(ggplot2);
library(ShortRead); 
library(ggplot2); 
library(phyloseq); 
library(Rmisc)
```

**Additonally we set the path to be the location of the unzipped fastq files**
```{r, eval=FALSE}
path <- "C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads" 
fns <- list.files(path)
```

**These are the files in our path, including all of the unzipped fastq files**
```{r}
fns
```

**Now we sort the files by forward and reverse sequence reads**

```{r}
fnFs <- sort(list.files(path, pattern = "_R1.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq", full.names = TRUE))
```

**After sorting, we can get the samples names. Note that name format should be SAMPLENAME_XXX.fastq**

```{r}
get.sample.name <- function(fname) strsplit(basename(fname), "_")[[1]][1]
sample.names <- unname(sapply(fnFs, get.sample.name))
sample.names
```
### Checking for Primers

**We checked for primers to ensure that we did not need to remove them from the reads**

These are our primers

```{r}
FWD <- "GTGAATTGCAGAACTCCGTG"
REV <- "CCTCCGCTTACTTATATGCTT" 
```

**Create all orientations of the primer sequences**
```{r, eval=FALSE}
allOrients <- function(primer) {
  require(Biostrings)
  dna <- DNAString(primer)
  orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
               RevComp = reverseComplement(dna))
  return(sapply(orients, toString))
  }

```

Then using the function set up above we create the orientations of our specific primer sequences in order to check all our sequences
```{r}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
REV.orients
```
Here we create a subdirectory of the filtered sequences for the next step to check for primers
```{r}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filterd files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = FALSE)
```
Now, we create a function to check for the presence of our forward or reverse primers in forward and reverse reads. 
```{r}
primerHits <- function(primer, fn) {
  nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
  return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[2]]), 
      FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[2]]), 
      REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[2]]), 
      REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[2]]))
```
We found no primers, so there is no need to remove them from our data before proceeding.

### Visualize Raw data

**Now we check the quality of the reads to determine at what point we should trim to avoid errors**

As our files are paired end we ran quality plots for both the forward and reverse reads. We had to remove sequences two sequences due to poor quality

#### Example of our foward read plots
```{r, eval=FALSE}
plotQualityProfile(fnFs[c(1,2,3,4,5,6,7,8,9)])
plotQualityProfile(fnFs[c(10,11,12,13,14,15)])
```

```{r,echo=FALSE}
plotQualityProfile(fnFs[c(1,8)])
```

#### Examples of our reverse read plots
```{r, eval=FALSE}
plotQualityProfile(fnRs[c(1,2,3,4,5,6,7,8,9)])
plotQualityProfile(fnRs[c(10,11,12,13,14,15)])
```

```{r, echo=FALSE}
plotQualityProfile(fnRs[c(1,8)])
```


**Make directory and filenames for the filtered fastqs**
```{r}
filt_path <- file.path(path, "trimmed")
if(!file_test("-d", filt_path)) dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(filt_path, paste0(sample.names, "_R_filt.fastq"))
```

Now, that we set the path we can actually filter our reads based on the quality plots. We are using the parameters set by Nicola as our data was from her. The truncLen values allow trimming to get better quality while maintaining overlap for later merging. We use these truncLen values as the quality of our reads drops off after about 220
```{r}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(220,200),
                     maxN=0,
                     maxEE=c(1,1),
                     truncQ=2, 
                     minLen = 50,
                     rm.phix=TRUE, 
                     matchIDs=TRUE, 
                     compress=TRUE, multithread=FALSE)
head(out)
tail(out)
```

### Error Rates
#DADA2 learns its error model from the data itself by alternating estimation of the error rates and the composition of the sample until they converge on a jointly consistent solution (this is similar to the E-M algorithm)
#As in many optimization problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

#setDadaOpt(MAX_CONSIST=30) #if necessary, increase number of cycles to allow convergence
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
```
#sanity check: visualize estimated error rates
#error rates should decline with increasing qual score
#red line is based on definition of quality score alone
#black line is estimated error rate after convergence
#dots are observed error rate for each quality score
```{r}
plotErrors(errF, nominalQ=TRUE) 
plotErrors(errR, nominalQ=TRUE) 
```
Error rates converge to zero with increasing Consensus Quality Score, as we had hoped

################################
##### Dereplicate reads #######
################################
#Dereplication combines all identical sequencing reads into “unique sequences” with a corresponding “abundance”: the number of reads with that unique sequence. 
#Dereplication substantially reduces computation time by eliminating redundant comparisons.
#DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent denoising step, significantly increasing DADA2’s accuracy.
```{r}
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
```
# Name the derep-class objects by the sample names
```{r}
names(derepFs) <- sample.names
names(derepRs) <- sample.names
```

################################
##### Infer Sequence Variants #######
################################
```{r}
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
```
The above code takes the dereplicated amplicon sequencing reads and returns the inferred composition of the samples, essentially removing sequencing errors.

#Now, we look at the dada class objects by sample
#This will tell us how many 'real' variants there are in unique input sequences
#By default, the dada function processes each sample independently, but pooled processing is available with pool=TRUE and that may give better results for low sampling depths at the cost of increased computation time.
```{r}
dadaFs[[1]]
dadaRs[[1]]
```

#~############################~#
##### Merge paired reads #######
#~############################~#

#To further cull spurious sequence variants
#Merge the denoised forward and reverse reads
#Paired reads that do not exactly overlap are removed

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

summary((mergers[[1]]))
```

#We now have a data.frame for each sample with the merged $sequence, its $abundance, and the indices of the merged $forward and $reverse denoised sequences. Paired reads that did not exactly overlap were removed by mergePairs.

#~##################################~#
##### Construct sequence table #######
#~##################################~#
#a higher-resolution version of the “OTU table” produced by classical methods

```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
rowSums(seqtab)
```

# Inspect distribution of sequence lengths
```{r}
table(nchar(getSequences(seqtab)))
```
Most reads were at 306 bp (73 datapoints), which is close to what was expected according to Nicola's data, which assumed a range of 271-303 bp. The plot below shows the frequency of these sequences by base pairs
```{r}
plot(table(nchar(getSequences(seqtab))))
```


#The sequence table is a matrix with rows corresponding to (and named by) the samples, and 
#columns corresponding to (and named by) the sequence variants. 
#Sequences that are much longer or shorter than expected may be the result of non-specific priming, and may be worth removing

#If we wanted to remove some lengths - not recommended by dada2 for its2 data
#seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(268,327)] 

```{r}
table(nchar(getSequences(seqtab)))
dim(seqtab)
plot(table(nchar(getSequences(seqtab))))
```

#construct sequence table
```{r}
seqtab <- makeSequenceTable(dadaFs)
head(seqtab)
```

################################
##### Remove chimeras #######
################################
#The core dada method removes substitution and indel errors, but chimeras remain. 
#Fortunately, the accuracy of the sequences after denoising makes identifying chimeras easier 
#than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as 
#a bimera (two-parent chimera) from more abundant sequences.
Here, we remove all chimeras and return chimera-free unique sequences 
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
#Identified 5 bimeras out of 93 input sequences
Now, we find the proportion of our reads that do not have chimeras. Here we find that 99.3% of our reads were free of chimeras.
```{r}
sum(seqtab.nochim)/sum(seqtab)
```
#0.9930478
#The fraction of chimeras varies based on factors including experimental procedures and sample complexity, but can be substantial. 


################################
##### Track Read Stats #######
################################
Here, we create a table to visualize our reads as they were filtered, denoised, merged, tabled, and filtered for chimeras
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(mergers, getN), rowSums(seqtab), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoised", "merged", "tabled", "nonchim")
rownames(track) <- sample.names
head(track)
tail(track)
```
Here we write this table into a .csv file
```{r}
write.csv(track,file="its2_reads.csv",row.names=TRUE,quote=FALSE)
```




################################
##### Assign Taxonomy #######
################################

#It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to classify sequence variants taxonomically. 
#DADA2 provides a native implementation of the RDP's naive Bayesian classifier. The assignTaxonomy function takes a set of sequences and a training set of taxonomically classified sequences, and outputs the taxonomic assignments with at least minBoot bootstrap confidence.
#Here, I have supplied a modified version of the GeoSymbio ITS2 database listing more taxonomic info as phyloseq requires (Franklin et al. 2012)
#For example: GeoSymbio data (taken from "all clades" at https://sites.google.com/site/geosymbio/downloads):
#>A1.1
#modified version for phyloseq looks like this instead:
#>Symbiodinium; Clade A; A1.1

This assigns a clade to each sequence using corresponding reads from the GeoSymbio database
```{r}
taxa <- assignTaxonomy(seqtab.nochim, "GeoSymbio_ITS2_LocalDatabase_verForPhyloseq.fasta",tryRC=TRUE,minBoot=70,verbose=TRUE)
unname(head(taxa))
```

The code below writes these objects as a file so we can restore and come back to it later, as needed
```{r}
saveRDS(seqtab.nochim, file="C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads/mrits2_seqtab.nochim.rds")
saveRDS(taxa, file="C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads/mrits2_taxa.rds")
```

```{r}
write.csv(seqtab.nochim, file="mrits2_seqtab.nochim.csv")
write.csv(taxa, file="C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads/mrits2_taxa.csv")
```

#### Reading in prior data files ####
```{r}
setwd("C:/Users/corey/Downloads/HW_1/its2_sids_rads/its2_sids_rads")
seqtab.nochim <- readRDS("mrits2_seqtab.nochim.rds")
taxa <- readRDS("mrits2_taxa.rds")
```


################################
##### handoff 2 phyloseq #######
################################

#import dataframe holding sample information
#have your samples in the same order as the seqtab file in the rows, variables as columns


#import dataframe holding sample information
```{r}
samdf<-read.csv("mrits_sampledata.csv")
head(samdf)
rownames(samdf) <- samdf$Sample
```


#phyloseq object with shorter names - doing this one instead of one above
```{r}
ids <- paste0("sq", seq(1, length(colnames(seqtab.nochim))))
```
#making output fasta file for lulu step & maybe other things, before giving new ids to sequences
#path='~/moorea_holobiont/mr_ITS2/mrits2.fasta'
#uniquesToFasta(seqtab.nochim, path, ids = ids, mode = "w", width = 20000)
```{r}
colnames(seqtab.nochim)<-ids
taxa2 <- cbind(taxa, rownames(taxa)) #retaining raw sequence info before renaming
rownames(taxa2)<-ids

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa2),
               sample_names(ps))
```
ps
```

#removing sample 87 from ps object, no data
ps_no87 <- subset_samples(ps, Sample != "87")
ps_no87

#removing 87 from other things
samdf.no87 <- samdf[-92,]
seqtab.no87 <- seqtab.nochim[-92,]

#Bar-plots
top90 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:90]
ps.top90 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top90 <- prune_taxa(top90, ps.top90)

plot_bar(ps.top90, x="Sample", fill="Class") 

#visusalize via counts rather than abundances:
plot_bar(ps, x = "sample", fill= "Class") #+ facet_wrap("tank")
#
#Obtain a csv file for the phyloseq data. Will give you the abundances for each sample and class. Useful for constructing the heatmap. Also, enables you to use ggplot, and construct more aesthetically pleasing plot.
psz <- psmelt(ps.top90)
write.csv(psz, file="Phyloseqoutputfinal.csv")
p <- ggplot(psz, aes(x=Sample, y=Abundance, fill=Class))
p + geom_bar(stat="identity", colour="black")
